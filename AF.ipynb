{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm import tqdm\n",
    "from data.discovery_con import LABELS\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice, \n",
    "    AutoTokenizer,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser\n",
    ")\n",
    "from generate import DatasetGenerate, AdversarialFiltering\n",
    "from config import decoding_options\n",
    "from utils import compute_metrics\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('data/ctrl_main.json')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "original_dataset = load_dataset('discovery', 'discovery', split='train[:7%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "valid_ds = load_dataset('discovery', 'discovery', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/ctrl_valid_1.json')\n",
    "gen_ds = Dataset.from_pandas(df)\n",
    "\n",
    "# original_dataset = load_dataset('discovery', 'discovery', split='train[:7%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 2609,\n",
       " 'label': 95,\n",
       " 'sentence1': 'the parties in favor of negotiation, on the other hand, survived their defeat, and drew some strength from the weakness of the parties opposed to it.',\n",
       " 'sentence2': 'In this party in favor of negotiation there were many intellectuals, whose perspicacity and depth of thought no longer need demonstration.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[len(gen_ds)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'the parties in favor of negotiation, on the other hand, survived their defeat, and drew some strength from the weakness of the parties opposed to it.',\n",
       " 'marker': 'naturally',\n",
       " 'ground_truth': 'In this party in favor of negotiation there were many intellectuals, whose perspicacity and depth of thought no longer need demonstration.',\n",
       " 'option_0': 'The negotiations were renewed at once',\n",
       " 'option_1': 'It was not long before the negotiations were resumed but they were conducted on very different principles from those which had guided them in the former instance At length it was agreed that the terms of surrender should be submitted to the arbitration of Great Britain and the United States with the assurance of their respective ministers that they would abide by the decision',\n",
       " 'option_2': 'It is with these two tendencies that the present controversy has been principally waged in the North and the South At a certain period the natural policy of the North was to treat propositions as unconditional propositions and the demands of the South as conditionalities'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_o = load_dataset('discovery', 'discovery', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 10,\n",
       " 'label': 149,\n",
       " 'sentence1': 'Both Brown and Pavarotti have passed on since this performance, though not as a direct result of it.',\n",
       " 'sentence2': 'This duet-which took place in Modena in 2002-somehow managed to put both outsized talents into one massively melodramatic song without spiraling out of control.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_o[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    prompt = examples['context'] + '</s>' + examples['marker']\n",
    "    choice_0, choice_1, choice_2 = str(examples['ground_truth']), str(examples['option_0']), str(examples['option_1'])\n",
    "    choice_3 = str(examples['option_2'])\n",
    "    choices = [choice_0, choice_1, choice_2, choice_3]\n",
    "    random.shuffle(choices)\n",
    "    encoding = tokenizer([prompt, prompt, prompt, prompt], choices, return_tensors='pt', max_length=96, padding='max_length', truncation=True)\n",
    "    encoding[\"label\"] = choices.index(choice_0)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained('roberta-large', num_labels=4)\n",
    "tokenizer = AutoTokenizer.from_pretrained('ctrl')\n",
    "# model = AutoModelForMultipleChoice.from_pretrained('roberta-large', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pregnancy': 168629,\n",
       " 'Christianity': 7675,\n",
       " 'Explain': 106423,\n",
       " 'Fitness': 63440,\n",
       " 'Saving': 63163,\n",
       " 'Ask': 27171,\n",
       " 'Ass': 95985,\n",
       " 'Joke': 163509,\n",
       " 'Questions': 45622,\n",
       " 'Thoughts': 49605,\n",
       " 'Retail': 52342,\n",
       " 'Feminism': 164338,\n",
       " 'Writing': 11992,\n",
       " 'Atheism': 192263,\n",
       " 'Netflix': 48616,\n",
       " 'Computing': 39639,\n",
       " 'Opinion': 43213,\n",
       " 'Alone': 44967,\n",
       " 'Funny': 58917,\n",
       " 'Gaming': 40358,\n",
       " 'Human': 4088,\n",
       " 'India': 1331,\n",
       " 'Joker': 77138,\n",
       " 'Diet': 36206,\n",
       " 'Legal': 11859,\n",
       " 'Norman': 4939,\n",
       " 'Tip': 72689,\n",
       " 'Weight': 52343,\n",
       " 'Movies': 46273,\n",
       " 'Running': 23425,\n",
       " 'Science': 2090,\n",
       " 'Horror': 37793,\n",
       " 'Confession': 60572,\n",
       " 'Finance': 12250,\n",
       " 'Politics': 16360,\n",
       " 'Scary': 191985,\n",
       " 'Support': 12654,\n",
       " 'Technologies': 32516,\n",
       " 'Teenage': 66160,\n",
       " 'Event': 32769,\n",
       " 'Learned': 67460,\n",
       " 'Notion': 182770,\n",
       " 'Wikipedia': 37583,\n",
       " 'Books': 6665,\n",
       " 'Extract': 76050,\n",
       " 'Confessions': 102701,\n",
       " 'Conspiracy': 75932,\n",
       " 'Links': 63674,\n",
       " 'Narcissus': 150425,\n",
       " 'Relationship': 54766,\n",
       " 'Relationships': 134796,\n",
       " 'Reviews': 41671,\n",
       " 'News': 4256,\n",
       " 'Translation': 26820,\n",
       " 'multilingual': 128406}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.control_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30520]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('pregnancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In 1991, the remains of Russian Tsar Nicholas II and his family \\\n",
    "(except for Alexei and Maria) are discovered. \\\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the \\\n",
    "remainder of the story. 1883 Western Siberia, \\\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic. \\\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his \\\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the \\\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of \\\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous, \\\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the remainder of the story. 1883 Western Siberia, a young Grigori Rasputin is asked by his father and a group of men to perform magic. Rasputin has a vision and denounces one of the men as a horse thief. Although his father initially slaps him for making such an accusation, Rasputin watches as the man is chased outside and beaten. Twenty years later, Rasputin sees a vision of the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous, with people, even a bishop, begging for his blessing. <eod> </s> <eos>\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[168629]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Pregnancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pregnancy': 168629,\n",
       " 'Christianity': 7675,\n",
       " 'Explain': 106423,\n",
       " 'Fitness': 63440,\n",
       " 'Saving': 63163,\n",
       " 'Ask': 27171,\n",
       " 'Ass': 95985,\n",
       " 'Joke': 163509,\n",
       " 'Questions': 45622,\n",
       " 'Thoughts': 49605,\n",
       " 'Retail': 52342,\n",
       " 'Feminism': 164338,\n",
       " 'Writing': 11992,\n",
       " 'Atheism': 192263,\n",
       " 'Netflix': 48616,\n",
       " 'Computing': 39639,\n",
       " 'Opinion': 43213,\n",
       " 'Alone': 44967,\n",
       " 'Funny': 58917,\n",
       " 'Gaming': 40358,\n",
       " 'Human': 4088,\n",
       " 'India': 1331,\n",
       " 'Joker': 77138,\n",
       " 'Diet': 36206,\n",
       " 'Legal': 11859,\n",
       " 'Norman': 4939,\n",
       " 'Tip': 72689,\n",
       " 'Weight': 52343,\n",
       " 'Movies': 46273,\n",
       " 'Running': 23425,\n",
       " 'Science': 2090,\n",
       " 'Horror': 37793,\n",
       " 'Confession': 60572,\n",
       " 'Finance': 12250,\n",
       " 'Politics': 16360,\n",
       " 'Scary': 191985,\n",
       " 'Support': 12654,\n",
       " 'Technologies': 32516,\n",
       " 'Teenage': 66160,\n",
       " 'Event': 32769,\n",
       " 'Learned': 67460,\n",
       " 'Notion': 182770,\n",
       " 'Wikipedia': 37583,\n",
       " 'Books': 6665,\n",
       " 'Extract': 76050,\n",
       " 'Confessions': 102701,\n",
       " 'Conspiracy': 75932,\n",
       " 'Links': 63674,\n",
       " 'Narcissus': 150425,\n",
       " 'Relationship': 54766,\n",
       " 'Relationships': 134796,\n",
       " 'Reviews': 41671,\n",
       " 'News': 4256,\n",
       " 'Translation': 26820,\n",
       " 'multilingual': 128406}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.control_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441b766601e7458e8a280eacb47b1b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "remove_column_names = ['option_0', 'option_1', 'option_2', 'ground_truth', 'marker', 'context']\n",
    "dataset = dataset.map(preprocess_function, remove_columns=remove_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = '/home/nlp/apex/experiment/roberta/',\n",
    "                                 per_device_train_batch_size=32, num_train_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=1.3175509821170222, metrics={'train_runtime': 136.7218, 'train_samples_per_second': 1.382, 'total_flos': 4912507270656000, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ctrl')\n",
    "model = AutoModelForCausalLM.from_pretrained('/home/nlp/apex/experiment/ctrl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset = DatasetGenerate(original_dataset, LABELS, tokenizer, model, decoding_options)af.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = AdversarialFiltering(generate_dataset, model, preds, decoding_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1791/1791 [00:00<00:00, 78335.66it/s]\n",
      "  1%|          | 11/1791 [00:51<2:18:09,  4.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8f92830b8e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_new_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/apex/commonsense-discourse/generate.py\u001b[0m in \u001b[0;36mgenerate_new_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"marker\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mgenerated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_synthetic_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apex/commonsense-discourse/generate.py\u001b[0m in \u001b[0;36mgenerate_synthetic_options\u001b[0;34m(self, idx, context)\u001b[0m\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m         \u001b[0mlen_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_text_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         clean_examples = self.cleanup_generated_examples(\n\u001b[1;32m    101\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apex/commonsense-discourse/generate.py\u001b[0m in \u001b[0;36mgenerate_from_model\u001b[0;34m(self, tokenized_context, original_text_length)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         greedy_output = self.model.generate(\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoding_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;31m# greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m    906\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# stop when there is a </s> in each sentence, or if we exceed the maximul length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0munfinished_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "af.generate_new_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
