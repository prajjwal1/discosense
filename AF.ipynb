{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from data.discovery_con import LABELS\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/nlp/apex/experiment/ctrl/checkpoint-8000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('data/discovery.py', 'discovery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "discovery_train_ds = load_dataset('data/discovery.py', 'discovery', split='train[:7%]')\n",
    "discovery_valid_ds = dataset[\"validation\"]\n",
    "discovery_test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109620"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discovery_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                         max_length=64,\n",
    "                                         padding='max_length',\n",
    "                                         return_length=True,\n",
    "                                         add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
    "tokenizer.add_special_tokens({'sep_token': '[SEP]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-7be6b9f7f762>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-7be6b9f7f762>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    cleaned_text = .replace('.','', 1).replace('@@','').strip()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DiscoveryDatasetGenerate():\n",
    "    \n",
    "    def __init__(self, dataset, labels, tokenizer, model, decoding_options):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.decoding_options = decoding_options\n",
    "        \n",
    "    def get_sentence_as_context(self, idx, sentence_order):\n",
    "        context = self.dataset[idx]\n",
    "        tokenized_context = self.tokenizer(self.labels[context['label']] + ' ' + context[sentence_order], \n",
    "                                           return_tensors=\"pt\")\n",
    "        original_text_length = len(self.labels[context['label']] + ' ' + context[sentence_order])\n",
    "        return tokenized_context, self.labels[context['label']], original_text_length\n",
    "    \n",
    "    def check_model_output(self, output_from_model, original_text_length):\n",
    "        if len(tokenizer.decode(output_from_model.squeeze(0), skip_special_tokens=True)[original_text_length:])<5:\n",
    "            print(\"detected an empty greedy output\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def generate_from_model(self, tokenized_context, original_text_length):\n",
    "        input_ids = tokenized_context['input_ids'].to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        greedy_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[0])\n",
    "        beam_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[1])\n",
    "        top_p_k_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[2])\n",
    " \n",
    "        # Sometimes the greedy output is empty, so replace it with top-p-k\n",
    "        if self.check_model_output(greedy_output, original_text_length):\n",
    "            greedy_output = model.generate(input_ids=input_ids, \n",
    "                                **self.decoding_options[2])\n",
    "        \n",
    "        \n",
    "        outputs.append(greedy_output)\n",
    "        outputs.append(beam_output)\n",
    "        outputs.append(top_p_k_output)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def cleanup_generated_examples(self, outputs, idx, len_context, marker):\n",
    "        example = {}\n",
    "        example['ground_truth'] = self.dataset[idx]['sentence2']\n",
    "        for i, sample_output in enumerate(outputs):\n",
    "            uncleaned_text = tokenizer.decode(sample_output.squeeze(0).tolist(), skip_special_tokens=True)[len_context:]\n",
    "            cleaned_text = uncleaned_text.replace('\\n', '').replace('\\xa0', '').replace('\\\\', '').replace(marker, '').strip()\n",
    "            \n",
    "            if '.' in cleaned_text:\n",
    "                prev_input_end_index = cleaned_text.index('.') # to remove context in generated output\n",
    "                cleaned_text = cleaned_text[prev_input_end_index:]\n",
    "\n",
    "            #           if '.' or '@@' in cleaned_text:\n",
    "            cleaned_text = .replace('.','', 1).replace('@@','').strip()\n",
    "            \n",
    "            if '`' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace('`', '').strip()\n",
    "            if '\"' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace('\"', '').strip()\n",
    "            if ']' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace(']', '').strip()\n",
    "            if '-' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace('-', '').strip()\n",
    "            if ')' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace(')', '').strip()\n",
    "            if '}' in cleaned_text:\n",
    "                cleaned_text = cleaned_text.replace('}', '').strip()\n",
    "                \n",
    "            example['option_' + str(i)] = cleaned_text\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    def generate_synthetic_options(self, idx, context='sentence1'):\n",
    "        tokenized_context, marker, original_text_length = self.get_sentence_as_context(idx, context)\n",
    "        len_context = len(self.dataset[idx][context])\n",
    "        outputs = self.generate_from_model(tokenized_context, original_text_length)\n",
    "        clean_examples = self.cleanup_generated_examples(outputs, idx, len_context, marker)        \n",
    "        return clean_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_options_0 = {'max_length': 64,\n",
    "                    'repetition_penalty': 1.2,\n",
    "                    'temperature': 0}\n",
    "\n",
    "decoding_options_1 = {'max_length': 64,\n",
    "                      'num_beams':5, \n",
    "                      'no_repeat_ngram_size':2, \n",
    "                      'early_stopping':True}\n",
    "\n",
    "decoding_options_2 = {'max_length': 64,\n",
    "                    'do_sample':True, \n",
    "                    'max_length':50, \n",
    "                    'top_k':50, \n",
    "                    'top_p':0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_options = []\n",
    "decoding_options.append(decoding_options_0)\n",
    "decoding_options.append(decoding_options_1)\n",
    "decoding_options.append(decoding_options_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_ds = DiscoveryDatasetGenerate(discovery_train_ds, LABELS, tokenizer, model, decoding_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/109620 [00:13<225:52:39,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/109620 [00:24<251:55:15,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/109620 [00:33<263:01:46,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/109620 [01:04<293:09:03,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/109620 [02:17<287:26:56,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/109620 [03:16<294:47:23,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/109620 [03:46<304:32:23, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/109620 [04:06<306:54:39, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 27/109620 [04:13<275:52:09,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/109620 [04:33<282:22:59,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected an empty greedy output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/109620 [05:57<302:00:13,  9.92s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a61217ed5697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'marker'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgenerated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscovery_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_synthetic_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msynthetic_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9017e6e53189>\u001b[0m in \u001b[0;36mgenerate_synthetic_options\u001b[0;34m(self, idx, context)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlen_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_text_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mclean_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_generated_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclean_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9017e6e53189>\u001b[0m in \u001b[0;36mcleanup_generated_examples\u001b[0;34m(self, outputs, idx, len_context, marker)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0muncleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncleaned_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\xa0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mprev_input_end_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to remove context in generated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#             if '.' or '@@' in cleaned_text:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = []\n",
    "\n",
    "for i in tqdm(range(len(discovery_train_ds))):\n",
    "    example = {}\n",
    "    values = discovery_train_ds[i]\n",
    "    example['context'] = values['sentence1']\n",
    "    example['marker'] = LABELS[values['label']]\n",
    "    generated_options = discovery_ds.generate_synthetic_options(i)\n",
    "    example.update(generated_options)\n",
    "    synthetic_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'They continued to dig noting that there were pick marks on the walls of the pit where someone before them had dug out the pit.',\n",
       " 'marker': 'curiously',\n",
       " 'ground_truth': 'Every ten feet they found a layer of logs.',\n",
       " 'option_0': 'The two men looked at each other and then they both turned their heads back towards the cave. I was surprised by this',\n",
       " 'option_1': 'S they noticed that the hole in the ceiling was not filled in with the way it had been when they had first entered the room. The only thing that was missing from the room was',\n",
       " 'option_2': 'It is unclear who built the structures. I guess it must'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"on the other hand should we use plastic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_on_cuda(tok_output):\n",
    "    for k, v in tok_output.items():\n",
    "        tok_output[k] = v.cuda()\n",
    "    return tok_output\n",
    "cuda_token = put_on_cuda(tokenizer(text, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'on the other hand should we use plastic any The When When other as other the other hand up other hand other hand other hand other the other hand up other hand up other hand other the other the other the other the other the other as other the other'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(input_ids = cuda_token['input_ids'], **decoding_options_2).tolist()[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/synthetic_discovery_100.json', 'w') as fout:\n",
    "    json.dump(synthetic_dataset , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discovery_train_ds.column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb\n",
    "coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/DialoGPT-large were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/DialoGPT-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('microsoft/DialoGPT-large', num_labels = len(LABELS))\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-large')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('microsoft/DialoGPT-large', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(example['sentence1'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([82])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
