{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from data.discovery_con import LABELS\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/nlp/apex/experiment/ctrl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('data/discovery.py', 'discovery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset discovery (/home/nlp/.cache/huggingface/datasets/discovery/discovery/1.0.0/f08ced5950fb93854c70d20fc70d1583766d7219cda67e65d197dfe9ec3775ca)\n"
     ]
    }
   ],
   "source": [
    "discovery_train_ds = load_dataset('data/discovery.py', 'discovery', split='train[:7%]')\n",
    "discovery_valid_ds = dataset[\"validation\"]\n",
    "discovery_test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109620"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discovery_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                         max_length=64,\n",
    "                                         padding='max_length',\n",
    "                                         return_length=True,\n",
    "                                         add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
    "tokenizer.add_special_tokens({'sep_token': '[SEP]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dasdsa dasas2321 '"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^A-Za-z0-9 ]+', '', 'dasdsa\\xad dasas2321 }{[]`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscoveryDatasetGenerate():\n",
    "    \n",
    "    def __init__(self, dataset, labels, tokenizer, model, decoding_options):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device).eval()\n",
    "        self.decoding_options = decoding_options\n",
    "        self.fixed_sequences = 0\n",
    "        \n",
    "    def get_sentence_as_context(self, idx, sentence_order):\n",
    "        context = self.dataset[idx]\n",
    "        tokenized_context = self.tokenizer(self.labels[context['label']] + ' ' + context[sentence_order], \n",
    "                                           return_tensors=\"pt\")\n",
    "        original_text_length = len(self.labels[context['label']] + ' ' + context[sentence_order])\n",
    "        return tokenized_context, self.labels[context['label']], original_text_length\n",
    "    \n",
    "    def check_model_output(self, output_from_model, original_text_length):\n",
    "        if len(tokenizer.decode(output_from_model.squeeze(0), skip_special_tokens=True)[original_text_length:])<5:\n",
    "            self.fixed_sequences += 1\n",
    "            print(\"Detected an empty greedy output. Count: \", self.fixed_sequences)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def generate_from_model(self, tokenized_context, original_text_length):\n",
    "        input_ids = tokenized_context['input_ids'].to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        greedy_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[0])\n",
    "        beam_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[1])\n",
    "        top_p_k_output = model.generate(input_ids=input_ids, \n",
    "                                 **self.decoding_options[2])\n",
    " \n",
    "        # Sometimes the greedy output is empty, so replace it with top-p-k\n",
    "        if self.check_model_output(greedy_output, original_text_length):\n",
    "            greedy_output, top_p_k_output = model.generate(input_ids=input_ids, \n",
    "                                **self.decoding_options[3])[:]\n",
    "        \n",
    "        \n",
    "        outputs.append(greedy_output)\n",
    "        outputs.append(beam_output)\n",
    "        outputs.append(top_p_k_output)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def cleanup_generated_examples(self, outputs, idx, len_context, marker):\n",
    "        example = {}\n",
    "        example['ground_truth'] = self.dataset[idx]['sentence2']\n",
    "        \n",
    "        for i, sample_output in enumerate(outputs):\n",
    "            text = tokenizer.decode(sample_output.squeeze(0).tolist(), skip_special_tokens=True)[len_context:]\n",
    "            if '.' in text:\n",
    "                prev_input_end_index = text.index('.') # remove context in generated output\n",
    "                text = text[prev_input_end_index:]\n",
    "            text = re.sub('[^A-Za-z0-9 ]+', '', text) # remove special characters\n",
    "            text = text.replace(marker, '') #2. remove marker\n",
    "            example['option_' + str(i)] = text\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    def generate_synthetic_options(self, idx, context='sentence1'):\n",
    "        tokenized_context, marker, original_text_length = self.get_sentence_as_context(idx, context)\n",
    "        len_context = len(self.dataset[idx][context])\n",
    "        outputs = self.generate_from_model(tokenized_context, original_text_length)\n",
    "        clean_examples = self.cleanup_generated_examples(outputs, idx, len_context, marker)        \n",
    "        return clean_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_options_0 = {'max_length': 64,\n",
    "                    'repetition_penalty': 1.2,\n",
    "                    'temperature': 0}\n",
    "\n",
    "decoding_options_1 = {'max_length': 64,\n",
    "                      'num_beams':5, \n",
    "                      'no_repeat_ngram_size':2, \n",
    "                      'early_stopping':True}\n",
    "\n",
    "decoding_options_2 = {'max_length': 64,\n",
    "                    'do_sample':True, \n",
    "                    'max_length':64, \n",
    "                    'top_k':50, \n",
    "                    'top_p':0.95}\n",
    "\n",
    "fallback_decoding = {'max_length': 64,\n",
    "                     'num_beams':25, \n",
    "                     'no_repeat_ngram_size':2,\n",
    "                     'num_return_sequences': 2,\n",
    "                     'temperature': 0.7,\n",
    "                     'early_stopping':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_options = []\n",
    "decoding_options.append(decoding_options_0)\n",
    "decoding_options.append(decoding_options_1)\n",
    "decoding_options.append(decoding_options_2)\n",
    "decoding_options.append(fallback_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_ds = DiscoveryDatasetGenerate(discovery_train_ds, LABELS, tokenizer, model, decoding_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/109620 [00:17<139:42:31,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected an empty greedy output. Count:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/109620 [00:37<172:29:53,  5.67s/it]"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = []\n",
    "\n",
    "for i in tqdm(range(len(discovery_train_ds))):\n",
    "    example = {}\n",
    "    values = discovery_train_ds[i]\n",
    "    example['context'] = values['sentence1']\n",
    "    example['marker'] = LABELS[values['label']]\n",
    "    generated_options = discovery_ds.generate_synthetic_options(i)\n",
    "    example.update(generated_options)\n",
    "    synthetic_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthetic_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7f250aa96cd0>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_options_0 = {'max_length': 64,\n",
    "                    'repetition_penalty': 1.2}\n",
    "\n",
    "decoding_options_1 = {'max_length': 64,\n",
    "                      'num_beams':5, \n",
    "                      'no_repeat_ngram_size':2, \n",
    "                      'early_stopping':True}\n",
    "\n",
    "decoding_options_2 = {'max_length': 64,\n",
    "                    'do_sample':True, \n",
    "                    'max_length':64, \n",
    "                    'top_k':50, \n",
    "                    'top_p':0.85}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"curiously they continued to dig noting that there were pick marks on the walls of the pit where someone before them had dug out the pit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_on_cuda(tok_output):\n",
    "    for k, v in tok_output.items():\n",
    "        tok_output[k] = v.cuda()\n",
    "    return tok_output\n",
    "cuda_token = put_on_cuda(tokenizer(text, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"curiously they continued to dig noting that there were pick marks on the walls of the pit where someone before them had dug out the pit. `` The pit was filled in with earth and there was no sign of anyone having been in it before they filled it in. '' `'was told that the\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(input_ids = cuda_token['input_ids'], **decoding_options_1).tolist()[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.generate(input_ids = cuda_token['input_ids'], **fallback_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/synthetic_discovery_1500.json', 'w') as fout:\n",
    "    json.dump(synthetic_dataset , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discovery_train_ds.column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb\n",
    "coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/DialoGPT-large were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/DialoGPT-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('microsoft/DialoGPT-large', num_labels = len(LABELS))\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-large')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('microsoft/DialoGPT-large', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(example['sentence1'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([82])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
